{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca54f25-70bb-4af1-b0e9-218cc21b5c6f",
   "metadata": {},
   "source": [
    "Library necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4de6fb-684c-4d23-97c4-1bcfbab69a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09665a9-167c-446a-af58-2ef767a1596f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'ASL/asl_alphabet_train/asl_alphabet_train'\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for label_name in os.listdir(base_dir):\n",
    "    label_dir = os.path.join(base_dir, label_name)\n",
    "    if not os.path.isdir(label_dir):\n",
    "        continue\n",
    "    \n",
    "    for fname in os.listdir(label_dir):\n",
    "        fpath = os.path.join(label_dir, fname)\n",
    "        img = Image.open(fpath).convert('L')  \n",
    "        img = img.resize((28, 28))           \n",
    "        arr = np.array(img).astype('float32') / 255.0\n",
    "        X_list.append(arr.flatten())        \n",
    "        y_list.append(label_name)       \n",
    "\n",
    "X = np.array(X_list)\n",
    "y = np.array(y_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cdb965-3ef2-4ea7-bcca-81b32a3ef086",
   "metadata": {},
   "source": [
    "Preprocessing for classic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6474852-7424-4655-af8f-5ae5dd94ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287d68e-4d77-425b-a525-685c93b0926d",
   "metadata": {},
   "source": [
    "Separate data into train set and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49704836-cf0f-4868-900c-beecd3f5223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (69600, 784) Test: (17400, 784)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y       \n",
    ")\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1436aa4c-342e-4d4f-bae7-7449de682428",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a9dcb-51e7-4707-a4e5-228ca3c2ea25",
   "metadata": {},
   "source": [
    "Function to evaluate models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5c6b4b2-b22b-4b3a-8a7f-66c190253b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X_train, X_test, y_train, y_test, name=\"Model\"):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\" {name}\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1 score (weighted):\", f1)\n",
    "    print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, cmap='Purples')\n",
    "    plt.xlabel(\"Predict\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.title(f\"Confusion Matrix â€“ {name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return model, y_pred, acc, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29b4ef-767a-44cf-86b9-d585a5954785",
   "metadata": {},
   "source": [
    "Classification models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e178e-c776-463f-aef7-62b074c3c52b",
   "metadata": {},
   "source": [
    "### Neural Network MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60017662-29ce-4e95-91c9-a2666f2dadf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef9ec4d8-0a21-4a26-961b-b2a44827099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train_std, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_std, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "test_dataset  = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f576e4e3-79a9-43a5-9ccc-04a755b0759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "mlp_model = MLP(X_train_std.shape[1], len(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8898168e-d764-4c72-94b0-c5b445e67c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1127.7514\n",
      "Epoch 2, Loss: 373.4659\n",
      "Epoch 3, Loss: 249.0966\n",
      "Epoch 4, Loss: 206.1787\n",
      "Epoch 5, Loss: 164.2864\n",
      "Epoch 6, Loss: 154.8046\n",
      "Epoch 7, Loss: 143.7789\n",
      "Epoch 8, Loss: 126.2417\n",
      "Epoch 9, Loss: 124.1588\n",
      "Epoch 10, Loss: 124.3544\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    mlp_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31074519-0b75-4b60-90fa-77e59aadad3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.9420689655172414\n",
      "MLP F1: 0.9420617264869597\n"
     ]
    }
   ],
   "source": [
    "mlp_model.eval()\n",
    "preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_loader:\n",
    "        outputs = mlp_model(xb)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        preds.extend(predicted.numpy())\n",
    "\n",
    "mlp_acc = accuracy_score(y_test, preds)\n",
    "mlp_f1  = f1_score(y_test, preds, average='weighted')\n",
    "\n",
    "print(\"MLP Accuracy:\", mlp_acc)\n",
    "print(\"MLP F1:\", mlp_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa658f-6dd4-4ba7-bb6a-9961bd5833dd",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "43623851-8de9-4b57-bb1d-7a815b08298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),    \n",
    "    transforms.RandomRotation(degrees=15)      \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d1ffaa12-48fd-4693-86dd-acb751fdaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.images[idx]    \n",
    "        y = self.labels[idx]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x) \n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "507b413a-dfb9-4524-889e-1a86d7717fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_img = X_train.reshape(-1, 1, img_size, img_size)\n",
    "X_test_img  = X_test.reshape(-1, 1, img_size, img_size)\n",
    "\n",
    "X_train_img_t = torch.tensor(X_train_img, dtype=torch.float32)\n",
    "X_test_img_t  = torch.tensor(X_test_img, dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_t  = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_cnn_ds = ASLDataset(X_train_img_t, y_train_t, transform=train_transform)\n",
    "test_cnn_ds  = ASLDataset(X_test_img_t, y_test_t, transform=None)\n",
    "\n",
    "train_cnn_loader = DataLoader(train_cnn_ds, batch_size=32, shuffle=True)\n",
    "test_cnn_loader  = DataLoader(test_cnn_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "358b7a58-f81c-4f21-af03-56d00e61013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*5*5, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "cnn_model = CNN(len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8af74a56-3a5b-49ff-8721-8b1b8bb4c79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Epoch 2 completed\n",
      "Epoch 3 completed\n",
      "Epoch 4 completed\n",
      "Epoch 5 completed\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    cnn_model.train()\n",
    "    \n",
    "    for xb, yb in train_cnn_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(xb)\n",
    "        loss = criterion(outputs, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "687e526b-9910-4ee9-a7c0-f961d0006c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Accuracy: 0.8397126436781609\n",
      "CNN F1: 0.8400347340139306\n"
     ]
    }
   ],
   "source": [
    "cnn_model.eval()\n",
    "cnn_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, _ in test_cnn_loader:\n",
    "        outputs = cnn_model(xb)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        cnn_preds.extend(predicted.numpy())\n",
    "\n",
    "cnn_acc = accuracy_score(y_test, cnn_preds)\n",
    "cnn_f1  = f1_score(y_test, cnn_preds, average='weighted')\n",
    "\n",
    "print(\"CNN Accuracy:\", cnn_acc)\n",
    "print(\"CNN F1:\", cnn_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4df1b7a0-343f-4527-a739-5678e8d01b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagen = \"Image23.png\"  \n",
    "\n",
    "img = Image.open(imagen).convert(\"L\")\n",
    "img = img.resize((img_size, img_size))\n",
    "img_array = np.array(img) / 255.0\n",
    "\n",
    "img_tensor = torch.tensor(img_array).unsqueeze(0).unsqueeze(0).float()\n",
    "\n",
    "plt.imshow(img_array, cmap=\"gray\")\n",
    "plt.title(\"External Test\") \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "811c7952-58b7-43eb-9736-bfbada22eed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction: H\n"
     ]
    }
   ],
   "source": [
    "classes=le.classes_\n",
    "cnn_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = cnn_model(img_tensor)\n",
    "    pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "print(\"Model Prediction:\", classes[pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4cadc556-bb77-4ddf-a56e-c65732a3f3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Image20.png'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1, 4, 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1fba9c-cd52-4ba4-804b-d59fa89b4d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (asl_ml)",
   "language": "python",
   "name": "asl_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
